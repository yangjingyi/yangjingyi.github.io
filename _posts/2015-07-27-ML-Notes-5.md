---
layout: post
title: Machine Learning Notes (5)
categories: [data science]
tags: [machine learning, Coursera]
description: Regularization for linear regression and logistic regression

---
- underfitting = high bias
- overfitting = high variance

To avoid the overfitting, we could use the **regularization** method.

## Linear regression

### Gradient descent
Cost function

\\[
J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)}))^2+\lambda \sum^n_{j=1}\theta_j^2
\\]

In the term added, notice that **j is from 1** instead of 0. $$\lambda$$ is the **regularization parameter**. A too large $$\lambda$$ will cause the underfitting since all $$\theta_j \simeq 0$$.

Gradient descent

\\[
 \theta_0:=\theta_0-\alpha\frac{\partial J}{\partial \theta_j} = \theta_0-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}) \\\
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha [\frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
\\]

### Normal equation

\\[
\theta=(X^TX+\begin{vmatrix}
0 & 0 & 0 & ... & 0 \\\
0 & \lambda & 0 & ... & 0 \\\
0 & 0 & \lambda & ... & 0  \\\
 & & & ... \\\
0 & 0 & 0 & ... & \lambda
\end{vmatrix})^{-1}X^Ty
\\]

The matrix added is an square matrix of dimension (n+1)

## Logistic regression
Cost function

\\[
J(\theta)=-\frac{1}{m}\sum\_{i=1}^m (y^{(i)}log(h\_\theta(x^{(i)}))+(1-y^{(i)})log(1-h\_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum\_{j=1}^n \theta_j^2
\\]

Gradient descent

\\[
\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum\_{i=1}^m (h\_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\
\theta_j = \theta_j - \alpha [\frac{1}{m} \sum\_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j]
\\]