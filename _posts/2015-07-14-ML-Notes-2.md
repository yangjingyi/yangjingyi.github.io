---
layout: post
title: Machine Learning Notes (2)
categories: [data science]
tags: [machine learning, Coursera]
description: Multiple variables linear regression using the gradient descent / feature scaling / several trick for the debugging

---
Having studied the univariable linear regression, we are going to extend the dimension from 1 to n. n is the number of the features that result in an output.

## Multiple variables
For a multivatiate linear regression, we take

\\[x =  \begin{vmatrix}
x_0 \\\
x_1 \\\
x_2 \\\
. \\\
. \\\
. \\\
x_n
\end{vmatrix} \quad \text{where } x_0=1 \\]


\\[\theta^T =  \begin{vmatrix}
\theta_0 &  \theta_1 & \theta_2 & ... & \theta_n
\end{vmatrix} \\]

The hypothesis function is 

$$ h_{\theta}(x)=\theta^Tx $$

Given the cost function 

\\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

we could deduce the gradient descent 

\\[
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}) 
\\]
 
## Feature scaling
The goal of feature scaling is to make sure the features are on a similar scale. It could accelerate the convergence  
**Mean normalization**

  \\[
  x_j:=\frac{x_j-\mu_j}{(max - min)}\quad \text{so} -0.5 \leq x_j \leq 0.5
  \\]
  
## Tricks for the debugging
- To make sure the gradient descent working correctly, with an increasing number of iteration, the J should decrease
- Choose the learning rate 0.001 -> 0.003 -> 0.01 -> 0.03 -> 0.1 -> 0.3 -> 1 
- Polynomial regression
  $$ h_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2=\theta_0+\theta_1x_1+\theta_2x_2 $$
  In this case, the feature scale becomes more important.
  However, the square function will finally decrease with a great x. To avoid that, we could choose another model
  
  $$ h_{\theta}(x)=\theta_0+\theta_1x+\theta_2\sqrt{x} $$
