---
layout: post
title: Machine Learning Notes (7)
categories: [data science]
tags: [machine learning, Coursera]
description: SVM
---

Apart from the logistic regression, Support Vector Machine is another powerful algorithm for classification problems.

##  Gaussian kernel

Knowing a training data set $$ x^{(1)},\ x^{(2)} ...,\ x^{(m)} $$, we put all these training examples to landmarks $$ l^{(i)}= x^{(i)}$$. The kernel function

\\[f^{(i)} =  \begin{vmatrix}
f^{(i)}_0 \\\
f^{(i)}_1 \\\
f^{(i)}_2 \\\
. \\\
. \\\
. \\\
f^{(i)}_m
\end{vmatrix} \quad \text{where } f_0=1 \\]

\\[
f^{(i)}_j=exp(-\frac{\|x-l^{(i)}\|^2}{2\sigma ^2})
\\]

Gaussian kernel is used when we have a large data set and a intermediate number of features.

Code Matlab to implement a gaussian kernel function

	function sim = gaussianKernel(x1, x2, sigma)
	    % Ensure that x1 and x2 are column vectors
	    x1 = x1(:); x2 = x2(:);
	    sim = exp(- (x1 - x2)' * (x1 - x2) / 2 / (sigma^2));   
	end

## Linear kernel

\\[f^{(i)} = x^{(i)}
\\]

Linear kernel is used when we have a large data set and a small number of features or a small data set and a large number of features. Logistic regression could work in such situations as well.

## SVM Training
__First, do perform feature scaling__

Second, train with the cost function :
\\[J = C\sum\_{i=1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+\frac{1}{2}\sum_{j=1}^{m}\theta_j^2
\\]

cost1 is a function like this

<img src="/images/svm-cost1.jpg" width="400px"/> 

while cost0 is 

<img src="/images/svm-cost0.jpg" width="400px"/>

- Choice of C (= $$\frac{1}{\lambda}$$)
  - a large C leads to a low bias and a high variance (overfitting) 
  - a small C leads to a high bias and a low variance (underfitting)

- Choice of $$\sigma^2$$
  - a large $$\sigma^2$$ leads to a high bias and a low variance (underfitting)  
  - a small $$\sigma^2$$ leads to a low bias and a high variance (overfitting)

Finally, to predict, we got
\\[
y = 1 \quad if\ \theta^Tf\geq0
\\]