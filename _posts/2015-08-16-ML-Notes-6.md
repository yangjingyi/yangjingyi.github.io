---
layout: post
title: Machine Learning Notes (6)
categories: [data science]
tags: [machine learning, Coursera]
description: BP neural network
---

To solve a classification problem with neural network, we usually do these following steps:

## 1. Pick a network architecture
- Number of input units = dimension of features
- Number of output units = number of classes
- the number of hidden layer and hidden units depends

## 2. Randomly initialize weights
Considering the bias unit, we add layer_in with 1. And we declare $$\epsilon$$ so that all elements in W are in $$[-\epsilon, \epsilon]$$

	W = rand(layer_out, 1 + layer_in) * 2 * epsilon - epsilon;
	
## 3. Forward propagation to compute $$a^{(L)}$$

In a neural network model, 
\\[
a^{(L)} = g((\theta^{(L-1)})^Ta^{(L-1)})
\\]
where g is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and $$a^{(L-1)}$$ contains a bias unit.

## 4. Compute the cost function J($$\theta$$)
The cost function of the neural network is :

$$
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\sum^K_{k=1}[y_k^{(i)}log(h_{\theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2
$$

- m is the number of training set
- K is the number of units in the output layer
- $$s_l$$ is the number of unit in layer $$l$$
- L is the total number of layers

When $$\lambda \neq 0$$, the function is regularized. We take no account of the bias units for the regularization.

	hidden = sigmoid([ones(m, 1) X] * Theta1');
	h_theta = sigmoid([ones(m, 1) hidden] * Theta2');
	term1 = (-y_code)' * log(h_theta);
	term2 = (1-y_code)' * log(1-h_theta);
	J = 1/m * (sum(term1(:)) - sum(term2(:)));

	reg_theta1 = Theta1(:, 2:end);
	reg_theta2 = Theta2(:, 2:end);
	reg_term = lambda / (2*m) * (reg_theta1(:)' * reg_theta1(:)...
                           + reg_theta2(:)' * reg_theta2(:));
	J = J + reg_term;

## 5. Back propagation to compute $$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}} $$

### for i = 1 to m

- First, get the values for all units

		% code for m training set
		a1 = [ones(m, 1), X];
		z2 = a1 * Theta1';
		a2 = [ones(m, 1), sigmoid(z2)];
		z3 = a2 * Theta2';
		a3 = sigmoid(z3);

- Second, define $$\delta_j^{(l)}$$ error of node j in layer l.  
For the output unit, $$\delta^{(L)}=a^{(L)}-y$$  
For l = (L-1), ..., 2, $$\delta^{(l)}=((\theta^{(l)})^T)\delta^{(l+1)}.*\underbrace{g'(z^{(l)})}_{a^{(l)}.*(1-a^{(l)})}$$ 

		% code for m training set
		delta3 = a3 - y_code;
		delta2 = delta3 * Theta2(:, 2:end) .* sigmoidGradient(z2);

- Third, suppose $$\Delta^{(l)} = \Delta^{(l)}+\delta^{(l+1)}a^{(l)T}$$
		
		% code for m training set
		Delta1 = delta2' * a1;
		Delta2 = delta3' * a2;
		
- Finally,   

$$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda \theta_{ij}^{(l)} \quad j\neq0$$

$$\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij} \quad j=0$$

## 6. Gradient checking
To make sure that the back propagation didn't go wrong, the method gradient checking is used.
Suppose e=[0 0 ... $$\epsilon$$ ... 0 0] that $$\epsilon$$ is the $$l^{th}$$ element.  
If 
$$
D^{(l)}\approx\frac{1}{2\epsilon}(J(\theta+e)-J(\theta-e))
$$, the propagation will work.


## 7. Minimize J
Since we have both the cost function and its partial derivatives, we can minimize J with gradient descent or other advanced optimization.