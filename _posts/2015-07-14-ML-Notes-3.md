---
layout: post
title: Machine Learning Notes (3)
categories: [data science]
tags: [machine learning, Coursera]
description: Normal equation for linear regression

---

## Normal equation
Apart from the gradient descent, normal equation is another method to find the minimum J analytically. From the last part, we know the cost function and the input x for one example of n features 

\\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

\\[x^{(i)} =  \begin{vmatrix}
x^{(i)}_0 \\\
x^{(i)}_1 \\\
x^{(i)}_2 \\\
. \\\
. \\\
. \\\
x^{(i)}_n
\end{vmatrix} \quad \text{where } x_0=1 \\]

Then we got a **design matrix** that is an m by (n+1) dimensional matrix

\\[X =  \begin{vmatrix}
... & ^tx^{(1)} & ... \\\
... & ^tx^{(2)} & ... \\\
... & ^tx^{(3)} & ... \\\
&. \\\
&. \\\
&. \\\
... & ^tx^{(m)} & ... 
\end{vmatrix} \\]

Whereas y is a vector of dimension m

\\[y =  \begin{vmatrix}
y^{(1)} \\\
y^{(2)} \\\
y^{(3)} \\\
. \\\
. \\\
. \\\
y^{(m)}
\end{vmatrix} \\]

To minimize the cost function, we calculate the $$\theta$$ from

\\[
\boxed{\theta=(X^TX)^{-1}X^Ty}
\\]

### Characteristics
- Advantages
  - no need to do the feature scaling
  - no need to choose the learning rate
  - no need to iterate

- Disadvantage
  - The calculation of the $$ (X^TX)^{-1} $$ could be very slow if n is large (>10000) since the complexity for a matrix n by n is $$ O(n^3) $$

If $$ (X^TX) $$ is non-invertible (singular / degenerate), it could be caused by 2 reasons:

- Redundant feature, which leads to linear dependences
- Too many features ($$m \leq n$$)

In the second case, we can delete some features or use the **regularization** that we'll talk about later.
