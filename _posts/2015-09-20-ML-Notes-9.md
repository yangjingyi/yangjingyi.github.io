---
layout: post
title: Machine Learning Notes (9)
categories: [data science]
tags: [machine learning, Coursera]
description: Dimensionality reduction / principal component analysis (PCA)
---

**Dimensionality reduction** is used to economise the memory / disk space, speed up learning algorithms and visualize high-dimensional data. However, it is not used to avoid overfitting.


## Principal component analysis

PCA reduces the data from n-D to k-D : find k vectors $$ u^{(1)}, ... , u^{(k)} $$ onto which to project the data so as to minimize the projection error.

### Algorithm

- Knowing the training set $$ x^{(1)}, ... , x^{(m)} \in \mathbb{R}^n $$

- Preprocessing (feature scaling / mean normalization)

\\[
\mu_j = \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)} \\\
\text{Replace } x_j \text{ with } x_j - \mu_j
\\]

if different feature on different scale

\\[
\text{Replace } x_j \text{ with } \frac{x_j - \mu_j}{s_j}
\\]

- Reduce data from n-D to k-D
  - Compute "covariance matrix" $$\Sigma = \frac{1}{m}\sum_{i=1}^m x^{(i)} (x^{(i)})^T$$
  - Compute "eigenvector" of $$\Sigma$$
  
            [U, S, V] = svd(Sigma);
  			
  	\\[
    U = \begin{vmatrix}
    | & | & ... & |\\\
    u^{(1)} & u^{(2)} & ... & u^{(n)} \\\
    | & | & ... & |
    \end{vmatrix}
   \\]
   Pick the first $$ k $$ column to form a new matrix $$U_{reduce}$$  
   New training set $$ z = U_{reduce}^T x $$  
   $$S$$ is a diagonal matrix with eigenvalues of $$ \Sigma $$ 
   
  - To choose a proper $$ k $$, we need	$$\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99 $$, 
  	that means 99% of variance is retained
  	
- Reconstruction from compressed representation : 
$$ x_{approx}^{(i)} = U_{reduce} z^{(i)} $$  
