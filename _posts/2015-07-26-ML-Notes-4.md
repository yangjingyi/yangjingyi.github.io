---
layout: post
title: Machine Learning Notes (4)
categories: [data science]
tags: [machine learning, Coursera]
description: Classification algorithm / logistic regression / optimization algorithms / multiclass classification / one-vs-all

---
## Classification
For the classifcation problem, the output y is no more continue. Instead, $$ y\in \{0, 1\} $$ for a **binary class classification** or $$ y\in \{1, 2, 3, ... , p\} $$ for a **multiclass classification**.

### Hypothesis function
\\[
h\_\theta (x)=g(\theta ^T x)=\frac{1}{1+e^{-\theta ^T x}} \quad 0\leq h_\theta (x) \leq 1
\\]  

This is a **sigmoid (or logistic) function** of $$ \theta ^T x $$   
<img src="/images/logisticfunction.jpg" width="400px"/>

#### Hypothesis representation
$$ h_\theta (x)=P(y=1|x; \theta) $$ represent the probability that $$y=1$$ given $$x$$ parameterized by $$\theta$$. So $$ P(y=0|x; \theta)=1-h_\theta (x) $$

#### Decision boundary
Observing the graph of the sigmoid function, it's clear that 
\\[
y = 1 \quad if\ h\_\theta (x) \geq 0.5 \Leftrightarrow \theta ^Tx \geq 0 \\\
y = 0 \quad if\ h_\theta (x) < 0.5 \Leftrightarrow \theta ^Tx < 0
\\]

For example, $$h_\theta = g(-3 + x_1 + x_2)$$, y=1 if $$-3 + x_1 + x_2 \geq 0$$
<img src="/images/decisionboundary.jpg" width="400px"/>  
The straight line is the **decision boundary** which is a property of the hypothesis and the parameters

A non-linear example: $$ h_\theta (x)=g(-1+x_1^2+x_2^2) $$  
<img src="/images/circledecisionboundary.jpg" width="400px"/>  

### Cost function
\\[
J(\theta)=\frac{1}{m}\sum\_{i=1}^m \underbrace{(-y^{(i)}log(h\_\theta(x^{(i)}))-(1-y^{(i)})log(1-h\_\theta(x^{(i)})))}\_{cost(h_\theta(x),\ y)}
\\]

### Gradient descent
\\[
\theta_j = \theta_j - \alpha \frac{1}{m} \sum\_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\\]

### Optimization algorithms
To speed up the computation, we can use the optimization algorithms in Matlab.  
First, implement the cost function: ```func[jVal, gradient]=costFunction(theta)```. jVal is the cost function $$J(\theta)$$ and gradient is a matrix 
$$
\begin{vmatrix}
\frac{\partial J}{\partial \theta_0} \\\
\frac{\partial J}{\partial \theta_1} \\\
. \\\
. \\\
. \\\
\frac{\partial J}{\partial \theta_n}
\end{vmatrix}
$$  
Second, call the function ```fminunc```. In this function $$dim(\theta) \geq 2$$

	options = optimset('GradObj', 'on', 'MaxIter', '100');
	initialTheta = zeros(n, 1);
	[optTheta, cost] = fminunc(@(t)(costFunction(t, X, y)), initialTheta, options);
	% optTheta is the optimized theta we need
	% cost should be near to 0
	% t the argement that calls the cost function, no need to declare before
	
## Multiclass classification
The algorithm we use here is called **one-vs-all**  
<img src="/images/one-vs-all.png" width="400px"/>  
For an input x, pick the class j that maxmizes $$h_\theta^{(j)}(x)$$