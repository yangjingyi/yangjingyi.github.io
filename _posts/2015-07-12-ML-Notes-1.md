---
layout: post
title: Machine Learning Notes (1)
categories: [data science]
tags: [machine learning, Coursera]
description: Introduction of the machine learning / univariable linear regression / cost function / gradient descent.

---
This is my personal note while watching the machine learning course on [Coursera](https://www.coursera.org "Coursera").

## Introduction of Machine Learning
The machine learning could be divided into 2 parts

### Supervised Learning
- Regression
- Classification (labeled)

### Unsupervised Learning
- Clustering the data

## Univariable linear regression
- Hypothesis function

  $$ h_{\theta}(x)=\theta_0 + \theta_1 x $$

- Cost function  
  The cost function for univariable linear regression is a [convex function](https://en.wikipedia.org/wiki/Convex_function) (or called bowl-shaped function). Therefore, it only has one global minimum point

  \\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \\]

- Gradient descend  
  $$ \alpha $$ is the learning rate that can't be too big or too small. A big rate could lead to a divergence instead of a convergence we expect. And a tiny rate would make the process too slow.
  
    $$ \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} $$

  Therefore we can conclude that

  $$ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(\theta_0 + \theta_1 x^{(i)} -y^{(i)}))^2 $$

  Deriving the function, we got

\\[\begin{aligned}
   \theta_0:=\theta_0-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h\_{\theta}(x^{(i)})-y^{(i)})) \\\
 \theta_1:=\theta_1-\alpha \frac{1}{m}\sum\_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}) 
 \end{aligned} \\]

  m - the size of the training examples  
  x - input  
  y - output  
  This method is called **Batch Gradient Descent** when we use all the training examples
