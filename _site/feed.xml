<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Stylish Jekyll Theme</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Machine Learning Notes (9)</title>
				<description>&lt;p&gt;&lt;strong&gt;Dimensionality reduction&lt;/strong&gt; is used to economise the memory / disk space, speed up learning algorithms and visualize high-dimensional data. However, it is not used to avoid overfitting.&lt;/p&gt;

&lt;h2 id=&quot;principal-component-analysis&quot;&gt;Principal component analysis&lt;/h2&gt;

&lt;p&gt;PCA reduces the data from n-D to k-D : find k vectors &lt;script type=&quot;math/tex&quot;&gt;u^{(1)}, ... , u^{(k)}&lt;/script&gt; onto which to project the data so as to minimize the projection error.&lt;/p&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Knowing the training set &lt;script type=&quot;math/tex&quot;&gt;x^{(1)}, ... , x^{(m)} \in \mathbb{R}^n&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Preprocessing (feature scaling / mean normalization)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[
\mu_j = \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)} \\
\text{Replace } x_j \text{ with } x_j - \mu_j
\]&lt;/p&gt;

&lt;p&gt;if different feature on different scale&lt;/p&gt;

&lt;p&gt;\[
\text{Replace } x_j \text{ with } \frac{x_j - \mu_j}{s_j}
\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reduce data from n-D to k-D
    &lt;ul&gt;
      &lt;li&gt;Compute “covariance matrix” &lt;script type=&quot;math/tex&quot;&gt;\Sigma = \frac{1}{m}\sum_{i=1}^m x^{(i)} (x^{(i)})^T&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Compute “eigenvector” of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;&lt;/p&gt;

        &lt;pre&gt;&lt;code&gt;      [U, S, V] = svd(Sigma);
&lt;/code&gt;&lt;/pre&gt;

        &lt;p&gt;\[
U = \begin{vmatrix}
| &amp;amp; | &amp;amp; … &amp;amp; |\\
u^{(1)} &amp;amp; u^{(2)} &amp;amp; … &amp;amp; u^{(n)} \\
| &amp;amp; | &amp;amp; … &amp;amp; |
\end{vmatrix}
 \]
 Pick the first &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; column to form a new matrix &lt;script type=&quot;math/tex&quot;&gt;U_{reduce}&lt;/script&gt;&lt;br /&gt;
 New training set &lt;script type=&quot;math/tex&quot;&gt;z = U_{reduce}^T x&lt;/script&gt;&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is a diagonal matrix with eigenvalues of &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;To choose a proper &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, we need	&lt;script type=&quot;math/tex&quot;&gt;\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99&lt;/script&gt;, 
  that means 99% of variance is retained&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reconstruction from compressed representation : 
&lt;script type=&quot;math/tex&quot;&gt;x_{approx}^{(i)} = U_{reduce} z^{(i)}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 20 Sep 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/09/20/ML-Notes-9.html</link>
				<guid isPermaLink="true">/data%20science/2015/09/20/ML-Notes-9.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (8)</title>
				<description>&lt;h2 id=&quot;k-means-algorithm&quot;&gt;K-Means algorithm&lt;/h2&gt;

&lt;p&gt;Inputs :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K (number of clusters)&lt;/li&gt;
  &lt;li&gt;Training set &lt;script type=&quot;math/tex&quot;&gt;{x^{(1)}, ... , x^{(m)}}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cost function :&lt;/p&gt;

&lt;p&gt;\[J = \frac{1}{m}\sum_{i=1}^{m}|x^{(i)}-\mu_{c^{(i)}}|^2
\]&lt;/p&gt;

&lt;p&gt;We could repeat the following process for 50 - 1000 times and pick the clustering that gives the lowest cost.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Ramdomly initialize K cluster centroids &lt;script type=&quot;math/tex&quot;&gt;\mu_1, ... ,\mu_k&lt;/script&gt;&lt;br /&gt;
Randomly pick K training examples&lt;br /&gt;
Set &lt;script type=&quot;math/tex&quot;&gt;\mu_1, ... ,\mu_k&lt;/script&gt; equal to these examples&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Repeat&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Cluster assignment step&lt;br /&gt;
 for i=1:m&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; = index of cluster centroid (1~K) closest to &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Move centroid&lt;br /&gt;
 for k=1:K&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\mu_k&lt;/script&gt; = mean of points assigned to cluster k&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Mon, 14 Sep 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/09/14/ML-Notes-8.html</link>
				<guid isPermaLink="true">/data%20science/2015/09/14/ML-Notes-8.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (7)</title>
				<description>&lt;p&gt;Apart from the logistic regression, Support Vector Machine is another powerful algorithm for classification problems.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-kernel&quot;&gt;Gaussian kernel&lt;/h2&gt;

&lt;p&gt;Knowing a training data set &lt;script type=&quot;math/tex&quot;&gt;x^{(1)},\ x^{(2)} ...,\ x^{(m)}&lt;/script&gt;, we put all these training examples to landmarks &lt;script type=&quot;math/tex&quot;&gt;l^{(i)}= x^{(i)}&lt;/script&gt;. The kernel function&lt;/p&gt;

&lt;p&gt;\[f^{(i)} =  \begin{vmatrix}
f^{(i)}_0 \\
f^{(i)}_1 \\
f^{(i)}_2 \\
. \\
. \\
. \\
f^{(i)}_m
\end{vmatrix} \quad \text{where } f_0=1 \]&lt;/p&gt;

&lt;p&gt;\[
f^{(i)}_j=exp(-\frac{|x-l^{(i)}|^2}{2\sigma ^2})
\]&lt;/p&gt;

&lt;p&gt;Gaussian kernel is used when we have a large data set and a intermediate number of features.&lt;/p&gt;

&lt;p&gt;Code Matlab to implement a gaussian kernel function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function sim = gaussianKernel(x1, x2, sigma)
    % Ensure that x1 and x2 are column vectors
    x1 = x1(:); x2 = x2(:);
    sim = exp(- (x1 - x2)&#39; * (x1 - x2) / 2 / (sigma^2));   
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;linear-kernel&quot;&gt;Linear kernel&lt;/h2&gt;

&lt;p&gt;\[f^{(i)} = x^{(i)}
\]&lt;/p&gt;

&lt;p&gt;Linear kernel is used when we have a large data set and a small number of features or a small data set and a large number of features. Logistic regression could work in such situations as well.&lt;/p&gt;

&lt;h2 id=&quot;svm-training&quot;&gt;SVM Training&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;First, do perform feature scaling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Second, train with the cost function :
\[J = C\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+\frac{1}{2}\sum_{j=1}^{m}\theta_j^2
\]&lt;/p&gt;

&lt;p&gt;cost1 is a function like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/svm-cost1.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;while cost0 is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/svm-cost0.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Choice of C (= &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\lambda}&lt;/script&gt;)
    &lt;ul&gt;
      &lt;li&gt;a large C leads to a low bias and a high variance (overfitting)&lt;/li&gt;
      &lt;li&gt;a small C leads to a high bias and a low variance (underfitting)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Choice of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;a large &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; leads to a high bias and a low variance (underfitting)&lt;/li&gt;
      &lt;li&gt;a small &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt; leads to a low bias and a high variance (overfitting)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, to predict, we got
\[
y = 1 \quad if\ \theta^Tf\geq0
\]&lt;/p&gt;
</description>
				<pubDate>Sat, 12 Sep 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/09/12/ML-Notes-7.html</link>
				<guid isPermaLink="true">/data%20science/2015/09/12/ML-Notes-7.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (6)</title>
				<description>&lt;p&gt;To solve a classification problem with neural network, we usually do these following steps:&lt;/p&gt;

&lt;h2 id=&quot;pick-a-network-architecture&quot;&gt;1. Pick a network architecture&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Number of input units = dimension of features&lt;/li&gt;
  &lt;li&gt;Number of output units = number of classes&lt;/li&gt;
  &lt;li&gt;the number of hidden layer and hidden units depends&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;randomly-initialize-weights&quot;&gt;2. Randomly initialize weights&lt;/h2&gt;
&lt;p&gt;Considering the bias unit, we add layer_in with 1. And we declare &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; so that all elements in W are in &lt;script type=&quot;math/tex&quot;&gt;[-\epsilon, \epsilon]&lt;/script&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W = rand(layer_out, 1 + layer_in) * 2 * epsilon - epsilon;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;forward-propagation-to-compute-al&quot;&gt;3. Forward propagation to compute &lt;script type=&quot;math/tex&quot;&gt;a^{(L)}&lt;/script&gt;&lt;/h2&gt;

&lt;p&gt;In a neural network model, 
\[
a^{(L)} = g((\theta^{(L-1)})^Ta^{(L-1)})
\]
where g is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;sigmoid function&lt;/a&gt; and &lt;script type=&quot;math/tex&quot;&gt;a^{(L-1)}&lt;/script&gt; contains a bias unit.&lt;/p&gt;

&lt;h2 id=&quot;compute-the-cost-function-jtheta&quot;&gt;4. Compute the cost function J(&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;)&lt;/h2&gt;
&lt;p&gt;The cost function of the neural network is :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=-\frac{1}{m}\sum^m_{i=1}\sum^K_{k=1}[y_k^{(i)}log(h_{\theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))_k)] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;m is the number of training set&lt;/li&gt;
  &lt;li&gt;K is the number of units in the output layer&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;s_l&lt;/script&gt; is the number of unit in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;L is the total number of layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;\lambda \neq 0&lt;/script&gt;, the function is regularized. We take no account of the bias units for the regularization.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hidden = sigmoid([ones(m, 1) X] * Theta1&#39;);
h_theta = sigmoid([ones(m, 1) hidden] * Theta2&#39;);
term1 = (-y_code)&#39; * log(h_theta);
term2 = (1-y_code)&#39; * log(1-h_theta);
J = 1/m * (sum(term1(:)) - sum(term2(:)));

reg_theta1 = Theta1(:, 2:end);
reg_theta2 = Theta2(:, 2:end);
reg_term = lambda / (2*m) * (reg_theta1(:)&#39; * reg_theta1(:)...
                       + reg_theta2(:)&#39; * reg_theta2(:));
J = J + reg_term;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;back-propagation-to-compute-fracpartial-jthetapartial-thetaijl-&quot;&gt;5. Back propagation to compute &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}&lt;/script&gt;&lt;/h2&gt;

&lt;h3 id=&quot;for-i--1-to-m&quot;&gt;for i = 1 to m&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, get the values for all units&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  % code for m training set
  a1 = [ones(m, 1), X];
  z2 = a1 * Theta1&#39;;
  a2 = [ones(m, 1), sigmoid(z2)];
  z3 = a2 * Theta2&#39;;
  a3 = sigmoid(z3);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Second, define &lt;script type=&quot;math/tex&quot;&gt;\delta_j^{(l)}&lt;/script&gt; error of node j in layer l.&lt;br /&gt;
For the output unit, &lt;script type=&quot;math/tex&quot;&gt;\delta^{(L)}=a^{(L)}-y&lt;/script&gt;&lt;br /&gt;
For l = (L-1), …, 2, &lt;script type=&quot;math/tex&quot;&gt;\delta^{(l)}=((\theta^{(l)})^T)\delta^{(l+1)}.*\underbrace{g&#39;(z^{(l)})}_{a^{(l)}.*(1-a^{(l)})}&lt;/script&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  % code for m training set
  delta3 = a3 - y_code;
  delta2 = delta3 * Theta2(:, 2:end) .* sigmoidGradient(z2);
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Third, suppose &lt;script type=&quot;math/tex&quot;&gt;\Delta^{(l)} = \Delta^{(l)}+\delta^{(l+1)}a^{(l)T}&lt;/script&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  % code for m training set
  Delta1 = delta2&#39; * a1;
  Delta2 = delta3&#39; * a2;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij}+\lambda \theta_{ij}^{(l)} \quad j\neq0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J(\theta)}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}=\frac{1}{m}\Delta^{(l)}_{ij} \quad j=0&lt;/script&gt;

&lt;h2 id=&quot;gradient-checking&quot;&gt;6. Gradient checking&lt;/h2&gt;
&lt;p&gt;To make sure that the back propagation didn’t go wrong, the method gradient checking is used.
Suppose e=[0 0 … &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; … 0 0] that &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is the &lt;script type=&quot;math/tex&quot;&gt;l^{th}&lt;/script&gt; element.&lt;br /&gt;
If 
&lt;script type=&quot;math/tex&quot;&gt;D^{(l)}\approx\frac{1}{2\epsilon}(J(\theta+e)-J(\theta-e))&lt;/script&gt;, the propagation will work.&lt;/p&gt;

&lt;h2 id=&quot;minimize-j&quot;&gt;7. Minimize J&lt;/h2&gt;
&lt;p&gt;Since we have both the cost function and its partial derivatives, we can minimize J with gradient descent or other advanced optimization.&lt;/p&gt;
</description>
				<pubDate>Sun, 16 Aug 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/08/16/ML-Notes-6.html</link>
				<guid isPermaLink="true">/data%20science/2015/08/16/ML-Notes-6.html</guid>
			</item>
		
			<item>
				<title>NumPy Tutorial (2)</title>
				<description>&lt;h3 id=&quot;where&quot;&gt;where&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;In [5]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
In [6]: yarr = xarr + 1
In [8]: cond = np.array([True, False, True, True, False])
In [9]: result = np.where(cond, xarr, yarr)
In [10]: result
Out[10]: array([ 1.1,  2.2,  1.3,  1.4,  2.5])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;xarr&lt;/code&gt; and &lt;code&gt;yarr&lt;/code&gt; could also be 2 scalar variables. With &lt;code&gt;where&lt;/code&gt; the matrix in the condition will be tested. Replace the elements passed with xarr (or the value at the same position if xarr is a matrix). Replace the ones failed with elements in yarr.&lt;/p&gt;

&lt;h3 id=&quot;tricks-for-the-boolean-array&quot;&gt;Tricks for the boolean array&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;% use sum to count the positive number
In [16]: arr = np.random.randn(100)
In [17]: (arr &amp;gt; 0).sum()
Out[17]: 50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;random&lt;/code&gt; is the lib for the generation of randomn numbers&lt;/p&gt;

&lt;h3 id=&quot;sort&quot;&gt;sort&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;arr.sort(0)&lt;/code&gt; sort each column
&lt;code&gt;arr.sort(1)&lt;/code&gt; sort each row&lt;/p&gt;
</description>
				<pubDate>Tue, 28 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/28/NumPy-Basic-2.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/28/NumPy-Basic-2.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (5)</title>
				<description>&lt;ul&gt;
  &lt;li&gt;underfitting = high bias&lt;/li&gt;
  &lt;li&gt;overfitting = high variance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To avoid the overfitting, we could use the &lt;strong&gt;regularization&lt;/strong&gt; method.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;Cost function&lt;/p&gt;

&lt;p&gt;\[
J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2+\lambda \sum^n_{j=1}\theta_j^2
\]&lt;/p&gt;

&lt;p&gt;In the term added, notice that &lt;strong&gt;j is from 1&lt;/strong&gt; instead of 0. &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; is the &lt;strong&gt;regularization parameter&lt;/strong&gt;. A too large &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; will cause the underfitting since all &lt;script type=&quot;math/tex&quot;&gt;\theta_j \simeq 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Gradient descent&lt;/p&gt;

&lt;p&gt;\[
 \theta_0:=\theta_0-\alpha\frac{\partial J}{\partial \theta_j} = \theta_0-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_0^{(i)}) \\
 \theta_j:=\theta_j-\alpha\frac{\partial J}{\partial \theta_j} = \theta_j-\alpha [\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
\]&lt;/p&gt;

&lt;h3 id=&quot;normal-equation&quot;&gt;Normal equation&lt;/h3&gt;

&lt;p&gt;\[
\theta=(X^TX+\begin{vmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; … &amp;amp; 0 \\
0 &amp;amp; \lambda &amp;amp; 0 &amp;amp; … &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \lambda &amp;amp; … &amp;amp; 0  \\
 &amp;amp; &amp;amp; &amp;amp; … \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; … &amp;amp; \lambda
\end{vmatrix})^{-1}X^Ty
\]&lt;/p&gt;

&lt;p&gt;The matrix added is an square matrix of dimension (n+1)&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;Cost function&lt;/p&gt;

&lt;p&gt;\[
J(\theta)=-\frac{1}{m}\sum_{i=1}^m (y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
\]&lt;/p&gt;

&lt;p&gt;Gradient descent&lt;/p&gt;

&lt;p&gt;\[
\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j = \theta_j - \alpha [\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j]
\]&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/27/ML-Notes-5.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/27/ML-Notes-5.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (4)</title>
				<description>&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;For the classifcation problem, the output y is no more continue. Instead, &lt;script type=&quot;math/tex&quot;&gt;y\in \{0, 1\}&lt;/script&gt; for a &lt;strong&gt;binary class classification&lt;/strong&gt; or &lt;script type=&quot;math/tex&quot;&gt;y\in \{1, 2, 3, ... , p\}&lt;/script&gt; for a &lt;strong&gt;multiclass classification&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hypothesis-function&quot;&gt;Hypothesis function&lt;/h3&gt;
&lt;p&gt;\[
h_\theta (x)=g(\theta ^T x)=\frac{1}{1+e^{-\theta ^T x}} \quad 0\leq h_\theta (x) \leq 1
\]&lt;/p&gt;

&lt;p&gt;This is a &lt;strong&gt;sigmoid (or logistic) function&lt;/strong&gt; of &lt;script type=&quot;math/tex&quot;&gt;\theta ^T x&lt;/script&gt; &lt;br /&gt;
&lt;img src=&quot;/images/logisticfunction.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;hypothesis-representation&quot;&gt;Hypothesis representation&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h_\theta (x)=P(y=1|x; \theta)&lt;/script&gt; represent the probability that &lt;script type=&quot;math/tex&quot;&gt;y=1&lt;/script&gt; given &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; parameterized by &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. So &lt;script type=&quot;math/tex&quot;&gt;P(y=0|x; \theta)=1-h_\theta (x)&lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;decision-boundary&quot;&gt;Decision boundary&lt;/h4&gt;
&lt;p&gt;Observing the graph of the sigmoid function, it’s clear that 
\[
y = 1 \quad if\ h_\theta (x) \geq 0.5 \Leftrightarrow \theta ^Tx \geq 0 \\
y = 0 \quad if\ h_\theta (x) &amp;lt; 0.5 \Leftrightarrow \theta ^Tx &amp;lt; 0
\]&lt;/p&gt;

&lt;p&gt;For example, &lt;script type=&quot;math/tex&quot;&gt;h_\theta = g(-3 + x_1 + x_2)&lt;/script&gt;, y=1 if &lt;script type=&quot;math/tex&quot;&gt;-3 + x_1 + x_2 \geq 0&lt;/script&gt;
&lt;img src=&quot;/images/decisionboundary.jpg&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
The straight line is the &lt;strong&gt;decision boundary&lt;/strong&gt; which is a property of the hypothesis and the parameters&lt;/p&gt;

&lt;p&gt;A non-linear example: &lt;script type=&quot;math/tex&quot;&gt;h_\theta (x)=g(-1+x_1^2+x_2^2)&lt;/script&gt;&lt;br /&gt;
&lt;img src=&quot;/images/circledecisionboundary.jpg&quot; width=&quot;400px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cost-function&quot;&gt;Cost function&lt;/h3&gt;
&lt;p&gt;\[
J(\theta)=\frac{1}{m}\sum_{i=1}^m \underbrace{(-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)})))}_{cost(h_\theta(x),\ y)}
\]&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;
&lt;p&gt;\[
\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\]&lt;/p&gt;

&lt;h3 id=&quot;optimization-algorithms&quot;&gt;Optimization algorithms&lt;/h3&gt;
&lt;p&gt;To speed up the computation, we can use the optimization algorithms in Matlab.&lt;br /&gt;
First, implement the cost function: &lt;code&gt;func[jVal, gradient]=costFunction(theta)&lt;/code&gt;. jVal is the cost function &lt;script type=&quot;math/tex&quot;&gt;J(\theta)&lt;/script&gt; and gradient is a matrix 
&lt;script type=&quot;math/tex&quot;&gt;\begin{vmatrix}
\frac{\partial J}{\partial \theta_0} \\\
\frac{\partial J}{\partial \theta_1} \\\
. \\\
. \\\
. \\\
\frac{\partial J}{\partial \theta_n}
\end{vmatrix}&lt;/script&gt;&lt;br /&gt;
Second, call the function &lt;code&gt;fminunc&lt;/code&gt;. In this function &lt;script type=&quot;math/tex&quot;&gt;dim(\theta) \geq 2&lt;/script&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;options = optimset(&#39;GradObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;);
initialTheta = zeros(n, 1);
[optTheta, cost] = fminunc(@(t)(costFunction(t, X, y)), initialTheta, options);
% optTheta is the optimized theta we need
% cost should be near to 0
% t the argement that calls the cost function, no need to declare before
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;multiclass-classification&quot;&gt;Multiclass classification&lt;/h2&gt;
&lt;p&gt;The algorithm we use here is called &lt;strong&gt;one-vs-all&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/images/one-vs-all.png&quot; width=&quot;400px&quot; /&gt;&lt;br /&gt;
For an input x, pick the class j that maxmizes &lt;script type=&quot;math/tex&quot;&gt;h_\theta^{(j)}(x)&lt;/script&gt;&lt;/p&gt;
</description>
				<pubDate>Sun, 26 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/26/ML-Notes-4.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/26/ML-Notes-4.html</guid>
			</item>
		
			<item>
				<title>NumPy Tutorial (1)</title>
				<description>&lt;h2 id=&quot;multidimensional-data-structure---ndarray&quot;&gt;Multidimensional data structure - ndarray&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;In [3]: import numpy as np
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;initiation&quot;&gt;Initiation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vector column&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [4]: vector = [1, 2, 3]
  In [5]: arr_vec = np.array(vector)
  In [6]: arr_vec
  Out[6]: array([1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;matrix&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [10]: matrix = [[1, 2, 3], [4, 5, 6]]
  In [11]: arr_mat = np.array(matrix)
  In [12]: arr_mat
  Out[12]: 
  array([[1, 2, 3],
         [4, 5, 6]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;others&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [13]: np.zeros(3)
  Out[13]: array([ 0.,  0.,  0.])
  In [14]: np.ones((2, 3))
  Out[14]: 
  array([[ 1.,  1.,  1.],
         [ 1.,  1.,  1.]])
  In [15]: np.eye(2)
  Out[15]: 
  array([[ 1.,  0.],
         [ 0.,  1.]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scalar-operations------&quot;&gt;Scalar operations (+ - * / **)&lt;/h3&gt;
&lt;p&gt;Instead of the multiplication between matrix, * is the multiplication between 2 elements of the same index&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [16]: arr_mat
Out[16]: 
array([[1, 2, 3],
       [4, 5, 6]])
In [17]: arr_mat * arr_mat
Out[17]: 
array([[ 1,  4,  9],
       [16, 25, 36]])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;indexing-and-slicing&quot;&gt;Indexing and slicing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;vector&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [19]: arr = np.arange(10)
  In [20]: arr
  Out[20]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
  In [21]: arr[3]
  Out[21]: 3
  In [22]: arr[0:3]
  # 0:3 means 0, 1, 2
  Out[22]: array([0, 1, 2])
  In [23]: arr[0:3] = 10
  In [24]: arr
  Out[24]: array([10, 10, 10,  3,  4,  5,  6,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;A slice is part of the array. If the value of a slice is changed, so will be the value in the original array&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [28]: slice = arr[5:7]
  In [29]: slice[1] = 0
  In [30]: arr
  Out[30]: array([10, 10, 10,  3,  4,  5,  0,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;But it could be copied if followed by &lt;code&gt;arr[5:7].copy()&lt;/code&gt;. Howerver, the code below won’t change the array either. We’ll talk about this later in the &lt;em&gt;Fancy indexing&lt;/em&gt; section.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [36]: slice = arr[0]
  In [37]: slice = 99
  In [38]: arr
  Out[38]: array([10, 10, 10,  3,  4,  5,  0,  7,  8,  9])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;matrix&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  In [44]: matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
  In [45]: arr2d = np.array(matrix)
  In [46]: arr2d[2]
  Out[46]: array([7, 8, 9])
  In [47]: arr2d[0][2]
  Out[47]: 3
  In [56]: arr2d[:2, 1:]
  # [0:2] and [1:max+1]
  Out[56]: 
  array([[2, 3],
         [5, 6]])
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fancy-indexing&quot;&gt;Fancy indexing&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;In [61]: new_arr = arr2d[[0, 2]]
In [62]: new_arr
Out[62]: 
array([[1, 2, 3],
       [7, 8, 9]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fancy indexing is different from the slicing. The fancy indexing is a copy of the value while the slicing is a copy of the reference.&lt;/p&gt;

&lt;h3 id=&quot;transpose&quot;&gt;Transpose&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;In [63]: new_arr.T
Out[63]: 
array([[1, 7],
       [2, 8],
       [3, 9]])
&lt;/code&gt;&lt;/pre&gt;

</description>
				<pubDate>Thu, 23 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/23/NumPy-Basic-1.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/23/NumPy-Basic-1.html</guid>
			</item>
		
			<item>
				<title>IntelliJ IDEA CE + Tomcat + Openshift 开发记录</title>
				<description>&lt;h3 id=&quot;section&quot;&gt;开发环境：&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;IntelliJ IDEA CE 14&lt;/li&gt;
  &lt;li&gt;Apache Tomcat 8.0.18&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-1&quot;&gt;项目目录&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Spage(Projet)
├─src
│ └─java
├─lib
├─doc
└─webRoot 
  ├─WEB-INF
  │ ├─classes
  │ ├─lib
  │ └─web.xml
  ├─dist （jar、war的存放路径）
  ├─css
  ├─js
  ├─view
  └─image
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-2&quot;&gt;开发流程&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;设置Java class文件输出目录。右键Module，Open module Settings，paths，将output path设为WEB-INF下的classes文件夹 &lt;strong&gt;注意，实在Module的Path选项卡里修改output path&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;将jsp-api-2.1.jar, servlet-api-2.5.jar拷贝到webRoot下。打包时需要的jar包放到lib目录。&lt;/li&gt;
  &lt;li&gt;右键Module，Open module Settings，Dependencies，+号引入刚刚拷贝进来的jar文件（和lib目录）&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;src目录建test Package，下建Test.java&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  public class Test extends HttpServlet {
      protected void doGet(HttpServletRequest req, HttpServletResponse res) throws ServletException, IOException{
          System.out.println(&quot;hellow world!&quot;);
          res.getWriter().println(&quot;Hellow world!&quot;);
      }
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在web.xml中注册该servlet&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&amp;gt;
  &amp;lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot;
   xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
   xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee
                http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot;
   version=&quot;3.1&quot;&amp;gt;
   &amp;lt;servlet&amp;gt;
   		&amp;lt;servlet-name&amp;gt;test&amp;lt;/servlet-name&amp;gt;
   		&amp;lt;servlet-class&amp;gt;test.Test&amp;lt;/servlet-class&amp;gt;
   	&amp;lt;/servlet&amp;gt;
   	&amp;lt;servlet-mapping&amp;gt;
   		&amp;lt;servlet-name&amp;gt;test&amp;lt;/servlet-name&amp;gt;
   		&amp;lt;url-pattern&amp;gt;/test&amp;lt;/url-pattern&amp;gt;
   	&amp;lt;/servlet-mapping&amp;gt;
   	&amp;lt;session-config&amp;gt;
   		&amp;lt;session-timeout&amp;gt;30&amp;lt;/session-timeout&amp;gt;
   	&amp;lt;/session-config&amp;gt;
  &amp;lt;/web-app&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;右键module，compile module&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在tomcat下 conf\catalina\localhost 创建xml文件（需要sudo）&lt;br /&gt;
&lt;strong&gt;注意，如果path=”/hello”，则文件并也要为hello.xml&lt;/strong&gt;&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  &amp;lt;?xml version=&quot;1.0&quot;?&amp;gt;
  &amp;lt;Context path=&quot;/hello&quot; docBase=&quot;/Users/Thierry/Desktop/Spage/Spage/webRoot&quot; debug=&quot;0&quot; privileged=&quot;true&quot;&amp;gt;
  &amp;lt;/Context&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;webRoot下建index.html&lt;/li&gt;
  &lt;li&gt;浏览器localhost:8080/hello中可以看到index.html的内容&lt;br /&gt;
浏览器localhost:8080/hello/test中可以触发test.java的内容&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;openshift&quot;&gt;Openshift部署&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;添加应用&lt;/li&gt;
  &lt;li&gt;checkout到本地&lt;/li&gt;
  &lt;li&gt;配置pom.xml文件&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;同步代码
以下以Spage项目为例&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt; projet-logiciel/
 ├─src/
 │  └─main/
 │     ├─java/
 │     │  ├─db/
 │     │  ├─spider/
 │     │  └─web/
 │     │     ├─SysContextListener.java
 │     │     └─Result.java
 │     ├─resources/
 │     └─webapp/
 │         ├─css/
 │         ├─img/
 │         ├─js/
 │         ├─WEB-INF/
 │         │  ├─lib/
 │         │  ├─tld/
 │         │  └─web.xml
 │         ├─index.html
 │         └─result.jsp
 └─pom.xml
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Wed, 15 Jul 2015 00:00:00 +0200</pubDate>
				<link>/web%20development/2015/07/15/Tomcat+Intellj+OpenShift.html</link>
				<guid isPermaLink="true">/web%20development/2015/07/15/Tomcat+Intellj+OpenShift.html</guid>
			</item>
		
			<item>
				<title>Machine Learning Notes (3)</title>
				<description>&lt;h2 id=&quot;normal-equation&quot;&gt;Normal equation&lt;/h2&gt;
&lt;p&gt;Apart from the gradient descent, normal equation is another method to find the minimum J analytically. From the last part, we know the cost function and the input x for one example of n features&lt;/p&gt;

&lt;p&gt;\[ J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}))^2 \]&lt;/p&gt;

&lt;p&gt;\[x^{(i)} =  \begin{vmatrix}
x^{(i)}_0 \\
x^{(i)}_1 \\
x^{(i)}_2 \\
. \\
. \\
. \\
x^{(i)}_n
\end{vmatrix} \quad \text{where } x_0=1 \]&lt;/p&gt;

&lt;p&gt;Then we got a &lt;strong&gt;design matrix&lt;/strong&gt; that is an m by (n+1) dimensional matrix&lt;/p&gt;

&lt;p&gt;\[X =  \begin{vmatrix}
… &amp;amp; ^tx^{(1)} &amp;amp; … \\
… &amp;amp; ^tx^{(2)} &amp;amp; … \\
… &amp;amp; ^tx^{(3)} &amp;amp; … \\
&amp;amp;. \\
&amp;amp;. \\
&amp;amp;. \\
… &amp;amp; ^tx^{(m)} &amp;amp; … 
\end{vmatrix} \]&lt;/p&gt;

&lt;p&gt;Whereas y is a vector of dimension m&lt;/p&gt;

&lt;p&gt;\[y =  \begin{vmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
. \\
. \\
. \\
y^{(m)}
\end{vmatrix} \]&lt;/p&gt;

&lt;p&gt;To minimize the cost function, we calculate the &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; from&lt;/p&gt;

&lt;p&gt;\[
\boxed{\theta=(X^TX)^{-1}X^Ty}
\]&lt;/p&gt;

&lt;h3 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Advantages
    &lt;ul&gt;
      &lt;li&gt;no need to do the feature scaling&lt;/li&gt;
      &lt;li&gt;no need to choose the learning rate&lt;/li&gt;
      &lt;li&gt;no need to iterate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Disadvantage
    &lt;ul&gt;
      &lt;li&gt;The calculation of the &lt;script type=&quot;math/tex&quot;&gt;(X^TX)^{-1}&lt;/script&gt; could be very slow if n is large (&amp;gt;10000) since the complexity for a matrix n by n is &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;(X^TX)&lt;/script&gt; is non-invertible (singular / degenerate), it could be caused by 2 reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redundant feature, which leads to linear dependences&lt;/li&gt;
  &lt;li&gt;Too many features (&lt;script type=&quot;math/tex&quot;&gt;m \leq n&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the second case, we can delete some features or use the &lt;strong&gt;regularization&lt;/strong&gt; that we’ll talk about later.&lt;/p&gt;
</description>
				<pubDate>Tue, 14 Jul 2015 00:00:00 +0200</pubDate>
				<link>/data%20science/2015/07/14/ML-Notes-3.html</link>
				<guid isPermaLink="true">/data%20science/2015/07/14/ML-Notes-3.html</guid>
			</item>
		
	</channel>
</rss>
